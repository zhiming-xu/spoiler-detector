{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Self-attentive Sentence Embedding\n",
    "\n",
    "The code is based on [gluon-nlp tutorial](https://gluon-nlp.mxnet.io/examples/sentence_embedding/self_attentive_sentence_embedding.html).\n",
    "\n",
    "## Import Related Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import multiprocessing as mp\n",
    "import gluonnlp as nlp\n",
    "\n",
    "from mxnet import gluon, nd, init\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import autograd, gluon, nd\n",
    "from d2l import try_gpu\n",
    "import pandas as pd\n",
    "\n",
    "# iUse sklearn's metric function to evaluate the results of the experiment\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# fixed random number seed\n",
    "np.random.seed(2333)\n",
    "mx.random.seed(2333)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "\n",
    "### Load Dataset\n",
    "\n",
    "See [dataloader](data_loader.ipynb) for how the training samples are obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 217, saw 9\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-76c0caf90dbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# load all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# create a list of review a label paris.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nrows'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 217, saw 9\n"
     ]
    }
   ],
   "source": [
    "data_folder = 'data/imdb/'\n",
    "file_name = 'train.csv'\n",
    "file_path = data_folder + file_name\n",
    "dev = False\n",
    "\n",
    "# load train file\n",
    "if dev:\n",
    "    # load only n rows\n",
    "    nrows = 5000\n",
    "    data = pd.read_csv(file_path, nrows=nrows, sep='|')\n",
    "else:\n",
    "    # load all\n",
    "    data = pd.read_csv(file_path, sep='|')\n",
    "\n",
    "# create a list of review a label paris.\n",
    "dataset = [[left, right, int(label)] for left, right, label in \\\n",
    "           zip(data['review_text'], data['plot_summary'], data['is_spoiler'])]\n",
    "\n",
    "# randomly divide one percent from the training set as a verification set.\n",
    "train_dataset, valid_dataset = nlp.data.train_valid_split(dataset)\n",
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "The purpose of the following code is to process the raw data so that the processed data can be used for model training and prediction. We will use the `SpacyTokenizer` to split the document into tokens, `ClipSequence` to crop the comments to the specified length, and build a vocabulary based on the word frequency of the training data. Then we attach the [Glove](https://nlp.stanford.edu/pubs/glove.pdf)[2]  pre-trained word vector to the vocabulary and convert each token into the corresponding word index in the vocabulary.\n",
    "Finally get the standardized training data set and verification data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The women are hot, things end there Yet another erotic thriller involving a web of lies and intrigue, this time around though there are no decent sex scenes to redeem it. Our protagonist must be one of the most lifeless human beings ever captured on celluloid. He has an incredibly good-looking wife that he never seems to look at or want to be with, but apparently he doesn't want to cheat on her with his co-worker either because damn it, the guy just really can't stand beautiful women. Back to incoherent rants about his work. The nudity is kept to an absolute minimum, which is pretty much the last thing you need in this sleazy kind of flick. Tawny Kitaen (or possibly her body double) has one brief nude scene, Shannon Whirry keeps her clothes on for the entire movie. That's just weird, is anyone really supposed to watch this for the gifted actors or the subtle writing? The ending is pretty inventive, I'll give it that. nan\n",
      "Go rent a playboy video I heard Tinto Brass did eroticism, fine. I heard he was a real artist using his film like a painting, fine. I can go on about the good things I've heard, now didn't I see s*** in this. A stylish porn? No, a boring porn that isn't really a porn more like odd voyeurism. The eroticism was so over bored it was more annoying and I'd say that it detracted from the plot, but there wasn't one. Had I wanted to watch a porno I'd'ave rented one. I expected a real movie with some nudity and sex not the other way around. Because there was so little and what there was was just so inane I'm just going to pretend like there weren't even characters. Now the cinematography, it was a bad spaghetti western quality. N ow I thought that Brass was supposed to be better than the spaghetti western directors, well he's actually worse. The shots were all bland and migraine worthy. I could go on. Then to really just make you hate the movie more the girl as the most irritating giggle I've ever heard (beside the fact it is a total sound bit)seriously after the first time I heard it I already wanted to punch her. I'll say this maybe his earlier stuff is better, but after this I really would have to doubt it. nan\n",
      "Erotic with an big E This must be one of the most erotic films ever made,if you like this , you must see the rest of Tinto Brass movies. Claudia Koll are playing an excellent roll, and she is SO sexy This film are very erotic , without going over the fine line between erotic and porn. I have seen and have most of the Tinto Brass erotic movies that are made , and I must say that I like that kind of movies where the ladies are natural and not filled up with silicon. So if you want an erotic ,sexy,hot movie, just get one of Tinto Brass and your evening will be saved. I have seen a lot of other erotic movies form all over the world, and I must say that I liked the erotic movies that came out in the USA in the 1970 was more erotic than the movies that are coming out today. todays movies have no humor. nan\n",
      "horrible male lead call me crass, but i preferred the higher titillation factor of \"Cheeky\", another anally fixated comedy from tinto brass...although \"all ladies do it\" has some witty moments (when compared to say, \"cheeky\"), they are destroyed by the hateful dubbing... i'm sure this movie has more appeal in its native language... next time someone transfers this to a boxed set, USE SUBTITLES! there are not enough genuinely erotic or explicit moments... it teases, to be sure, but the centerpiece of the movie is distinctly uninviting... a huge carnal disco on the outskirts of town, with enough freak factor to fuel the warhol factory... unfortunately, the synthesis of 80's dance music and bopping italians may make for some good laughs, but little excitement...the story is virtually identical to \"cheeky\" and \"monella\"... the bored wife of an \"uptight\" man (in brass world, anyone who doesn't get excited by his wife cheating on him) is intrigued by all the sexual tension around her, and decides to live in a state of hedonism... she does maintain that she reserves her love for her husband, but it takes awhile for him to come around to the idea...the main male character is horrible to watch... during the sex scenes, he grits his teeth, strains his neck muscles, and basically freaks out... not something you'd like to witness...once again, brass uses all his trademarks... mirror shots, overheads of black and white corridors, seduction in the toilet, urination, plenty of derri√®re, and laughable philosophy... normally i would smile and accept his vision of an impossibly ripe world, but this movie somehow disappoints, even with low expectations... some crisp and bold colours though... tinto could make a fortune filming washing powder ads... nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkPoolWorker-11:\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-3-8033a98f60cb>\", line 19, in preprocess\n",
      "    left, right = length_clip_review(tokenizer(left.lower())), \\\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/data/transforms.py\", line 324, in __call__\n",
      "    return [tok.text for tok in self._nlp(sample)]\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/data/transforms.py\", line 324, in <listcomp>\n",
      "    return [tok.text for tok in self._nlp(sample)]\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-3-8033a98f60cb>\", line 19, in preprocess\n",
      "    left, right = length_clip_review(tokenizer(left.lower())), \\\n",
      "  File \"<ipython-input-3-8033a98f60cb>\", line 19, in preprocess\n",
      "    left, right = length_clip_review(tokenizer(left.lower())), \\\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/data/transforms.py\", line 324, in __call__\n",
      "    return [tok.text for tok in self._nlp(sample)]\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/data/transforms.py\", line 324, in __call__\n",
      "    return [tok.text for tok in self._nlp(sample)]\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/spacy/language.py\", line 382, in __call__\n",
      "    doc = self.make_doc(text)\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/spacy/language.py\", line 406, in make_doc\n",
      "    return self.tokenizer(text)\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/data/transforms.py\", line 324, in <listcomp>\n",
      "    return [tok.text for tok in self._nlp(sample)]\n",
      "  File \"tokenizer.pyx\", line 112, in spacy.tokenizer.Tokenizer.__call__\n",
      "Process ForkPoolWorker-10:\n",
      "KeyboardInterrupt\n",
      "  File \"tokenizer.pyx\", line 169, in spacy.tokenizer.Tokenizer._tokenize\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"tokenizer.pyx\", line 248, in spacy.tokenizer.Tokenizer._attach_tokens\n",
      "Process ForkPoolWorker-9:\n",
      "  File \"vocab.pyx\", line 138, in spacy.vocab.Vocab.get\n",
      "Process ForkPoolWorker-8:\n",
      "  File \"<ipython-input-3-8033a98f60cb>\", line 19, in preprocess\n",
      "    left, right = length_clip_review(tokenizer(left.lower())), \\\n",
      "  File \"vocab.pyx\", line 167, in spacy.vocab.Vocab._new_lexeme\n",
      "Process ForkPoolWorker-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/data/transforms.py\", line 324, in __call__\n",
      "    return [tok.text for tok in self._nlp(sample)]\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/spacy/lang/lex_attrs.py\", line 177, in lower\n",
      "    def lower(string):\n",
      "Process ForkPoolWorker-7:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-12:\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/data/transforms.py\", line 324, in <listcomp>\n",
      "    return [tok.text for tok in self._nlp(sample)]\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-3-8033a98f60cb>\", line 19, in preprocess\n",
      "    left, right = length_clip_review(tokenizer(left.lower())), \\\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-3-8033a98f60cb>\", line 19, in preprocess\n",
      "    left, right = length_clip_review(tokenizer(left.lower())), \\\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-3-8033a98f60cb>\", line 20, in preprocess\n",
      "    length_clip_plot(tokenizer(right.lower()))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/data/transforms.py\", line 324, in __call__\n",
      "    return [tok.text for tok in self._nlp(sample)]\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/spacy/language.py\", line 382, in __call__\n",
      "    doc = self.make_doc(text)\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/spacy/language.py\", line 406, in make_doc\n",
      "    return self.tokenizer(text)\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-3-8033a98f60cb>\", line 19, in preprocess\n",
      "    left, right = length_clip_review(tokenizer(left.lower())), \\\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"tokenizer.pyx\", line 112, in spacy.tokenizer.Tokenizer.__call__\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/data/transforms.py\", line 324, in __call__\n",
      "    return [tok.text for tok in self._nlp(sample)]\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"tokenizer.pyx\", line 169, in spacy.tokenizer.Tokenizer._tokenize\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/data/transforms.py\", line 324, in <listcomp>\n",
      "    return [tok.text for tok in self._nlp(sample)]\n",
      "  File \"<ipython-input-3-8033a98f60cb>\", line 20, in preprocess\n",
      "    length_clip_plot(tokenizer(right.lower()))\n",
      "  File \"<ipython-input-3-8033a98f60cb>\", line 19, in preprocess\n",
      "    left, right = length_clip_review(tokenizer(left.lower())), \\\n",
      "  File \"tokenizer.pyx\", line 248, in spacy.tokenizer.Tokenizer._attach_tokens\n",
      "KeyboardInterrupt\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/data/transforms.py\", line 324, in __call__\n",
      "    return [tok.text for tok in self._nlp(sample)]\n",
      "  File \"vocab.pyx\", line 138, in spacy.vocab.Vocab.get\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/data/transforms.py\", line 324, in __call__\n",
      "    return [tok.text for tok in self._nlp(sample)]\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/spacy/language.py\", line 382, in __call__\n",
      "    doc = self.make_doc(text)\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/spacy/language.py\", line 382, in __call__\n",
      "    doc = self.make_doc(text)\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/spacy/language.py\", line 406, in make_doc\n",
      "    return self.tokenizer(text)\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"tokenizer.pyx\", line 112, in spacy.tokenizer.Tokenizer.__call__\n",
      "  File \"vocab.pyx\", line 167, in spacy.vocab.Vocab._new_lexeme\n",
      "  File \"tokenizer.pyx\", line 169, in spacy.tokenizer.Tokenizer._tokenize\n",
      "  File \"<ipython-input-3-8033a98f60cb>\", line 20, in preprocess\n",
      "    length_clip_plot(tokenizer(right.lower()))\n",
      "  File \"tokenizer.pyx\", line 248, in spacy.tokenizer.Tokenizer._attach_tokens\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/spacy/language.py\", line 406, in make_doc\n",
      "    return self.tokenizer(text)\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/data/transforms.py\", line 324, in __call__\n",
      "    return [tok.text for tok in self._nlp(sample)]\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/data/transforms.py\", line 324, in __call__\n",
      "    return [tok.text for tok in self._nlp(sample)]\n",
      "  File \"tokenizer.pyx\", line 89, in spacy.tokenizer.Tokenizer.__call__\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/spacy/language.py\", line 382, in __call__\n",
      "    doc = self.make_doc(text)\n",
      "  File \"vocab.pyx\", line 138, in spacy.vocab.Vocab.get\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/spacy/language.py\", line 382, in __call__\n",
      "    doc = self.make_doc(text)\n",
      "  File \"<ipython-input-3-8033a98f60cb>\", line 20, in preprocess\n",
      "    length_clip_plot(tokenizer(right.lower()))\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/spacy/language.py\", line 406, in make_doc\n",
      "    return self.tokenizer(text)\n",
      "  File \"doc.pyx\", line 194, in spacy.tokens.doc.Doc.__init__\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/spacy/language.py\", line 406, in make_doc\n",
      "    return self.tokenizer(text)\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/data/transforms.py\", line 324, in __call__\n",
      "    return [tok.text for tok in self._nlp(sample)]\n",
      "  File \"doc.pyx\", line 73, in spacy.tokens.doc._get_chunker\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/spacy/lang/en/lex_attrs.py\", line 57, in like_num\n",
      "    if text.lower() in _num_words:\n",
      "  File \"vocab.pyx\", line 167, in spacy.vocab.Vocab._new_lexeme\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/spacy/language.py\", line 382, in __call__\n",
      "    doc = self.make_doc(text)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/spacy/lang/lex_attrs.py\", line 181, in prefix\n",
      "    def prefix(string):\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/data/transforms.py\", line 324, in __call__\n",
      "    return [tok.text for tok in self._nlp(sample)]\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/spacy/language.py\", line 406, in make_doc\n",
      "    return self.tokenizer(text)\n",
      "  File \"tokenizer.pyx\", line 89, in spacy.tokenizer.Tokenizer.__call__\n",
      "KeyboardInterrupt\n",
      "  File \"doc.pyx\", line 194, in spacy.tokens.doc.Doc.__init__\n",
      "  File \"tokenizer.pyx\", line 112, in spacy.tokenizer.Tokenizer.__call__\n",
      "  File \"tokenizer.pyx\", line 169, in spacy.tokenizer.Tokenizer._tokenize\n",
      "  File \"tokenizer.pyx\", line 248, in spacy.tokenizer.Tokenizer._attach_tokens\n",
      "  File \"vocab.pyx\", line 138, in spacy.vocab.Vocab.get\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/gluonnlp/data/transforms.py\", line 324, in <listcomp>\n",
      "    return [tok.text for tok in self._nlp(sample)]\n",
      "  File \"vocab.pyx\", line 167, in spacy.vocab.Vocab._new_lexeme\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/spacy/lang/lex_attrs.py\", line 177, in lower\n",
      "    def lower(string):\n",
      "  File \"doc.pyx\", line 73, in spacy.tokens.doc._get_chunker\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/spacy/util.py\", line 61, in get_lang_class\n",
      "    entry_point = get_entry_point(\"spacy_languages\", lang)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/spacy/util.py\", line 260, in get_entry_point\n",
      "    for entry_point in pkg_resources.iter_entry_points(key):\n",
      "  File \"/home/steven/miniconda3/envs/dl/lib/python3.7/site-packages/pkg_resources/__init__.py\", line 656, in <genexpr>\n",
      "    for entry in dist.get_entry_map(group).values()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8033a98f60cb>\u001b[0m in \u001b[0;36mpreprocess_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Each sample is processed in an asynchronous manner.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrayDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrayDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    267\u001b[0m         '''\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8033a98f60cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Preprocess the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-8033a98f60cb>\u001b[0m in \u001b[0;36mpreprocess_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Each sample is processed in an asynchronous manner.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrayDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrayDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/multiprocessing/util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, wr, _finalizer_registry, sub_debug, getpid)\u001b[0m\n\u001b[1;32m    187\u001b[0m                 sub_debug('finalizer calling %s with args %s and kwargs %s',\n\u001b[1;32m    188\u001b[0m                           self._callback, self._args, self._kwargs)\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weakref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_terminate_pool\u001b[0;34m(cls, taskqueue, inqueue, outqueue, pool, worker_handler, task_handler, result_handler, cache)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'joining task handler'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask_handler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m             \u001b[0mtask_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'joining result handler'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tokenizer takes as input a string and outputs a list of tokens.\n",
    "tokenizer = nlp.data.SpacyTokenizer('en')\n",
    "\n",
    "# length_clip takes as input a list and outputs a list with maximum length 300.\n",
    "length_clip_review = nlp.data.ClipSequence(300)\n",
    "length_clip_plot = nlp.data.ClipSequence(100)\n",
    "\n",
    "def preprocess(x):\n",
    "\n",
    "    # now the first element in tuple is review, second plot and third label\n",
    "    try:\n",
    "        left, right, label = x[0], x[1], int(x[2])\n",
    "        assert(type(left)==type('str') and type(right)==type('str'))\n",
    "        assert(label==0 or label==1)\n",
    "    except:\n",
    "        print(left, right)\n",
    "        left, right=str(left), str(right)\n",
    "    # clip the length of review words\n",
    "    left, right = length_clip_review(tokenizer(left.lower())), \\\n",
    "                  length_clip_plot(tokenizer(right.lower()))\n",
    "    return left, right, label\n",
    "\n",
    "def get_length(x):\n",
    "    return float(len(x[0]))\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    start = time.time()\n",
    "\n",
    "    with mp.Pool() as pool:\n",
    "        # Each sample is processed in an asynchronous manner.\n",
    "        dataset = gluon.data.ArrayDataset(pool.map(preprocess, dataset))\n",
    "        lengths = gluon.data.ArrayDataset(pool.map(get_length, dataset))\n",
    "    end = time.time()\n",
    "\n",
    "    print('Done! Tokenizing Time={:.2f}s, #Sentences={}'.format(end - start, len(dataset)))\n",
    "    return dataset, lengths\n",
    "\n",
    "# Preprocess the dataset\n",
    "train_dataset, train_data_lengths = preprocess_dataset(train_dataset)\n",
    "valid_dataset, valid_data_lengths = preprocess_dataset(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocab\n",
    "train_seqs = [sample[0]+sample[1] for sample in train_dataset]\n",
    "counter = nlp.data.count_tokens(list(itertools.chain.from_iterable(train_seqs)))\n",
    "\n",
    "vocab = nlp.Vocab(counter, max_size=20000)\n",
    "\n",
    "# load pre-trained embedding, Glove\n",
    "embedding_weights = nlp.embedding.GloVe(source='glove.twitter.27B.200d')\n",
    "vocab.set_embedding(embedding_weights)\n",
    "print(vocab)\n",
    "\n",
    "# NOTE: to use the same encoder, we need to ensure that two inputs are of the same length\n",
    "# this is achieved by manual padding\n",
    "def token_to_idx(x):\n",
    "    return vocab[x[0]], vocab[x[1]], x[2]\n",
    "\n",
    "# A token index or a list of token indices is returned according to the vocabulary.\n",
    "with mp.Pool() as pool:\n",
    "    train_dataset = pool.map(token_to_idx, train_dataset)\n",
    "    valid_dataset = pool.map(token_to_idx, valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = vocab.embedding.token_to_idx['xmen']\n",
    "idx, vocab.embedding.idx_to_vec[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketing and DataLoader\n",
    "Since each sentence may have a different length, we need to use `Pad` to fill the sentences in a minibatch to equal lengths so that the data can be quickly tensored on the GPU. At the same time, we need to use `Stack` to stack the category tags of a batch of data. For convenience, we use `Tuple` to combine `Pad` and `Stack`.\n",
    "\n",
    "In order to make the length of the sentence pad in each minibatch as small as possible, we should make the sentences with similar lengths in a batch as much as possible. In light of this, we consider constructing a sampler using `FixedBucketSampler`, which defines how the samples in a dataset will be iterated in a more economic way.\n",
    "\n",
    "Finally, we use `DataLoader` to build a data loader for the training. dataset and validation dataset. The training dataset requires FixedBucketSampler, but the validation dataset doesn't require the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "bucket_num = 10\n",
    "bucket_ratio = 0.5\n",
    "\n",
    "\n",
    "def get_dataloader():\n",
    "    # Construct the DataLoader Pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0), \\\n",
    "                                          nlp.data.batchify.Pad(axis=0),\n",
    "                                          nlp.data.batchify.Stack())\n",
    "\n",
    "    # n this example, we use a FixedBucketSampler,\n",
    "    # which assigns each data sample to a fixed bucket based on its length.\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_data_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "\n",
    "    # train_dataloader\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    # valid_dataloader\n",
    "    valid_dataloader = gluon.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "train_dataloader, valid_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# experiement one the two dataloaders\n",
    "# for left, right, label in train_dataloader:\n",
    "    # print(left.shape, right.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSoftmaxCE(nn.HybridBlock):\n",
    "    def __init__(self, sparse_label=True, from_logits=False,  **kwargs):\n",
    "        super(WeightedSoftmaxCE, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.sparse_label = sparse_label\n",
    "            self.from_logits = from_logits\n",
    "\n",
    "    def hybrid_forward(self, F, pred, label, class_weight, depth=None):\n",
    "        if self.sparse_label:\n",
    "            label = F.reshape(label, shape=(-1, ))\n",
    "            label = F.one_hot(label, depth)\n",
    "        if not self.from_logits:\n",
    "            pred = F.log_softmax(pred, -1)\n",
    "\n",
    "        weight_label = F.broadcast_mul(label, class_weight)\n",
    "        loss = -F.sum(pred * weight_label, axis=-1)\n",
    "\n",
    "        # return F.mean(loss, axis=0, exclude=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignAttention(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AlignAttention, self).__init__(**kwargs)\n",
    "    \n",
    "    def forward(self, inp_left, inp_right):\n",
    "        # input dimension is of (batch_size, seq_len, embed_size)\n",
    "        # att dimension is of (batch_size, seq_len_left, seq_len_right)\n",
    "        att = nd.batch_dot(inp_left, nd.transpose(inp_right, axes = (0, 2, 1)))\n",
    "        # inp_left_dot dimention is of (batch_size, seq_left, embed_size)\n",
    "        inp_left_dot = nd.batch_dot(nd.softmax(att, axis=-1), inp_right)\n",
    "        # inp_right_dot dimension is of (batch_size, seq_right, embed_size)\n",
    "        inp_right_dot = nd.batch_dot(nd.softmax(nd.transpose(att, axes=(0, 2, 1)), axis=-1), inp_left)\n",
    "        # concat original (lstm output, dot multiplier, substraction, elementwise product)\n",
    "        # therefore, the real size is (batch_size, seq_len_left/right, embed_size*4)\n",
    "        aug_left = nd.concat(inp_left, inp_left_dot, inp_left-inp_left_dot, inp_left*inp_left_dot, dim=-1)\n",
    "        aug_right = nd.concat(inp_right, inp_right_dot, inp_right-inp_right_dot, inp_right*inp_right_dot, dim=-1)\n",
    "        return aug_left, aug_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedSeqInfer(nn.Block):\n",
    "    def __init__(self, vocab_len, embsize, nhidden, nlayers, nfc, nclass, drop_prob, **kwargs):\n",
    "        super(EnhancedSeqInfer, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # dropout prob\n",
    "            self.drop_prob = drop_prob\n",
    "            # word embedding\n",
    "            self.embedding_layer = nn.Embedding(vocab_len, embsize)\n",
    "            # first lstm, from sentence embed to hidden outputs\n",
    "            self.bilstm1 = rnn.LSTM(nhidden, num_layers=nlayers, dropout=drop_prob, bidirectional=True)\n",
    "            # second lstm, from augmented embed to m\n",
    "            self.bilstm2 = rnn.LSTM(nhidden, num_layers=1, dropout=drop_prob, bidirectional=True)\n",
    "            # enhancement\n",
    "            self.align_att = AlignAttention()\n",
    "            # this layer is used to output the final class\n",
    "            self.output_layer = nn.HybridSequential()\n",
    "            self.output_layer.add(nn.Dense(nfc, activation='tanh'), nn.Dropout(rate=drop_prob), \\\n",
    "                                  nn.Dense(nfc, activation='tanh'), nn.Dropout(rate=drop_prob), \\\n",
    "                                  nn.Dense(nclass))\n",
    "\n",
    "    def forward(self, inp_left, inp_right):\n",
    "        # inp is a list containing left_text and right_text\n",
    "        # their size: [batch, token_idx]\n",
    "        # inp_embed_left/right size: [batch, seq_len, embed_size]\n",
    "        inp_embed_left = self.embedding_layer(inp_left)\n",
    "        inp_embed_right = self.embedding_layer(inp_right)\n",
    "        # rnn requires the first dimension to be the time steps, output is (seq_len, batch_size, embed_size)\n",
    "        h_output_left = self.bilstm1(nd.transpose(inp_embed_left, axes=(1, 0, 2)))\n",
    "        h_output_right = self.bilstm1(nd.transpose(inp_embed_right, axes=(1, 0, 2)))\n",
    "        m_left, m_right = self.align_att(nd.transpose(h_output_left, axes=(1, 0, 2)), \\\n",
    "                                                      nd.transpose(h_output_right, axes=(1, 0, 2)))\n",
    "        # apply another layer of lstm\n",
    "        # v_left/right shape is (seq_len, batch_size, embed_size)\n",
    "        v_left = self.bilstm2(nd.transpose(m_left, axes=(1, 0, 2)))\n",
    "        v_right = self.bilstm2(nd.transpose(m_right, axes=(1, 0, 2)))\n",
    "        # restore v's shape (batch_size, seq_len, embed_size)\n",
    "        v_left = nd.transpose(v_left, axes=(1, 0, 2))\n",
    "        v_right = nd.transpose(v_right, axes=(1, 0, 2))\n",
    "        # apply max pooling 1D and avg pooling 1D\n",
    "        v_left_avg = nd.sum(v_left, axis=1) / v_left.shape[1]\n",
    "        v_right_avg = nd.sum(v_right, axis=1) / v_right.shape[1]\n",
    "        v_left_max = nd.max(v_left, axis=1)\n",
    "        v_right_max = nd.max(v_right, axis=1)\n",
    "        # concatenate these 4 matrices\n",
    "        dense_input = nd.concat(v_left_avg, v_left_max, v_right_avg, v_left_max, dim=-1)\n",
    "        \n",
    "        output = self.output_layer(dense_input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure parameters and build models\n",
    "\n",
    "The resulting `M` is a matrix, and the way to classify this matrix is `flatten`, `mean` or `prune`. Prune is a way of trimming parameters proposed in the original paper and has been implemented here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(vocab)\n",
    "emsize = 200   # word embedding size\n",
    "nhidden = 300    # lstm hidden_dim\n",
    "nlayers = 3     # lstm layers\n",
    "\n",
    "# final fc layer's number of hidden units and predicted number of classes\n",
    "nfc = 1024\n",
    "nclass = 2\n",
    "\n",
    "drop_prob = 0\n",
    "\n",
    "ctx = try_gpu()\n",
    "\n",
    "model = EnhancedSeqInfer(vocab_len, emsize, nhidden, nlayers, nfc, nclass, drop_prob)\n",
    "\n",
    "model.initialize(init=init.Xavier(), ctx=ctx)\n",
    "\n",
    "# Attach a pre-trained glove word vector to the embedding layer\n",
    "model.embedding_layer.weight.set_data(vocab.embedding.idx_to_vec)\n",
    "# fixed the embedding layer\n",
    "model.embedding_layer.collect_params().setattr('grad_req', 'null')\n",
    "\n",
    "print(model)\n",
    "\n",
    "train_curve, valid_curve = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(x_left, x_right, y, model, loss, class_weight):\n",
    "    pred = model(x_left, x_right)\n",
    "    y = nd.array(y.asnumpy().astype('int32')).as_in_context(ctx)\n",
    "    if loss_name in ['sce', 'l1', 'l2']:\n",
    "        l = loss(pred, y)\n",
    "    elif loss_name == 'wsce':\n",
    "        l = loss(pred, y, class_weight, class_weight.shape[0])\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "    return pred, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(data_iter, model, loss, trainer, ctx, is_train, epoch, \\\n",
    "              clip=None, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    loss_val = 0.\n",
    "    total_pred = []\n",
    "    total_true = []\n",
    "    n_batch = 0\n",
    "\n",
    "    for batch_x_left, batch_x_right, batch_y in data_iter:\n",
    "        batch_x_left = batch_x_left.as_in_context(ctx)\n",
    "        batch_x_right = batch_x_right.as_in_context(ctx)\n",
    "        batch_y = batch_y.as_in_context(ctx)\n",
    "\n",
    "        if is_train:\n",
    "            with autograd.record():\n",
    "                batch_pred, l = calculate_loss(batch_x_left, batch_x_right, \\\n",
    "                                               batch_y, model, loss, class_weight)\n",
    "\n",
    "            # backward calculate\n",
    "            l.backward()\n",
    "\n",
    "            # clip gradient\n",
    "            clip_params = [p.data() for p in model.collect_params().values()]\n",
    "            if clip is not None:\n",
    "                norm = nd.array([0.0], ctx)\n",
    "                for param in clip_params:\n",
    "                    if param.grad is not None:\n",
    "                        norm += (param.grad ** 2).sum()\n",
    "                norm = norm.sqrt().asscalar()\n",
    "                if norm > clip:\n",
    "                    for param in clip_params:\n",
    "                        if param.grad is not None:\n",
    "                            param.grad[:] *= clip / norm\n",
    "\n",
    "            # update parmas\n",
    "            trainer.step(batch_x_left.shape[0])\n",
    "\n",
    "        else:\n",
    "            batch_pred, l = calculate_loss(batch_x_left, batch_x_right, \\\n",
    "                                           batch_y, model, loss, class_weight)\n",
    "\n",
    "        # keep result for metric\n",
    "        batch_pred = nd.argmax(nd.softmax(batch_pred, axis=1), axis=1).asnumpy()\n",
    "        batch_true = np.reshape(batch_y.asnumpy(), (-1, ))\n",
    "        total_pred.extend(batch_pred.tolist())\n",
    "        total_true.extend(batch_true.tolist())\n",
    "        \n",
    "        batch_loss = l.mean().asscalar()\n",
    "\n",
    "        n_batch += 1\n",
    "        loss_val += batch_loss\n",
    "\n",
    "        # check the result of traing phase\n",
    "        if is_train and n_batch % 400 == 0:\n",
    "            print('epoch %d, batch %d, batch_train_loss %.4f, batch_train_acc %.3f' %\n",
    "                  (epoch, n_batch, batch_loss, accuracy_score(batch_true, batch_pred)))\n",
    "\n",
    "    # metric\n",
    "    F1 = f1_score(np.array(total_true), np.array(total_pred), average='binary')\n",
    "    acc = accuracy_score(np.array(total_true), np.array(total_pred))\n",
    "    loss_val /= n_batch\n",
    "\n",
    "    if is_train:\n",
    "        print('epoch %d, learning_rate %.5f \\n\\t train_loss %.4f, acc_train %.3f, F1_train %.3f, ' %\n",
    "              (epoch, trainer.learning_rate, loss_val, acc, F1))\n",
    "        train_curve.append((acc, F1))\n",
    "        # declay lr\n",
    "        if epoch % 3 == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.9)\n",
    "    else:\n",
    "        print('\\t valid_loss %.4f, acc_valid %.3f, F1_valid %.3f, ' % (loss_val, acc, F1))\n",
    "        valid_curve.append((acc, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid(data_iter_train, data_iter_valid, model, loss, trainer, \\\n",
    "                ctx, nepochs, clip=None, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    for epoch in range(1, nepochs+1):\n",
    "        start = time.time()\n",
    "        # train\n",
    "        is_train = True\n",
    "        one_epoch(data_iter_train, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, clip, class_weight, loss_name)\n",
    "\n",
    "        # valid\n",
    "        is_train = False\n",
    "        one_epoch(data_iter_valid, model, loss, trainer, ctx, is_train,\n",
    "                  epoch, clip, class_weight, loss_name)\n",
    "        end = time.time()\n",
    "        print('time %.2f sec' % (end-start))\n",
    "        print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Now that we are training the model, we use WeightedSoftmaxCE to alleviate the problem of data category imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = None\n",
    "loss_name = 'sce'\n",
    "optim = 'adam'\n",
    "lr = 0.001\n",
    "clip = .5\n",
    "nepochs = 10\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), optim, {'learning_rate': lr})\n",
    "\n",
    "if loss_name == 'sce':\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "elif loss_name == 'wsce':\n",
    "    loss = WeightedSoftmaxCE()\n",
    "    # the value of class_weight is obtained by counting data in advance. It can be seen as a hyperparameter.\n",
    "    class_weight = nd.array([1., 3.], ctx=ctx)\n",
    "elif loss_name == 'l1':\n",
    "    loss = gluon.loss.L1Loss()\n",
    "elif loss_name == 'l2':\n",
    "    loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train and valid\n",
    "train_valid(train_dataloader, valid_dataloader, model, loss, \\\n",
    "            trainer, ctx, nepochs, clip=clip, class_weight=class_weight, loss_name=loss_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "Now we will randomly input a movie plot summary and its review to see if the model decide the review is a spoiler or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Parameter 'embedding0_weight' is missing in file 'model/att-0001.params', which contains parameters: 'selfattentivebilstm2_selfattentivebilstm12_embedding0_weight', 'selfattentivebilstm2_selfattentivebilstm12_lstm0_l0_i2h_weight', 'selfattentivebilstm2_selfattentivebilstm12_lstm0_l0_h2h_weight', ..., 'selfattentivebilstm2_selfattentivebilstm12_selfattention0_dense1_weight', 'selfattentivebilstm2_selfattentivebilstm12_selfattention0_dense1_bias', 'selfattentivebilstm2_selfattentivebilstm12_dense0_weight', 'selfattentivebilstm2_selfattentivebilstm12_dense0_bias'. Please make sure source and target networks have the same prefix.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-d635b9d25ffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model/att-0001.params'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m '''\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/gluon/block.py\u001b[0m in \u001b[0;36mload_parameters\u001b[0;34m(self, filename, ctx, allow_missing, ignore_extra)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mloaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             self.collect_params().load(\n\u001b[0;32m--> 386\u001b[0;31m                 filename, ctx, allow_missing, ignore_extra, self.prefix)\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dl/lib/python3.7/site-packages/mxnet/gluon/parameter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, filename, ctx, allow_missing, ignore_extra, restore_prefix)\u001b[0m\n\u001b[1;32m    909\u001b[0m                     \u001b[0;34m\"Parameter '%s' is missing in file '%s', which contains parameters: %s. \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                     \"Please make sure source and target networks have the same prefix.\"%(\n\u001b[0;32m--> 911\u001b[0;31m                         name[lprefix:], filename, _brief_print_list(arg_dict.keys()))\n\u001b[0m\u001b[1;32m    912\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Parameter 'embedding0_weight' is missing in file 'model/att-0001.params', which contains parameters: 'selfattentivebilstm2_selfattentivebilstm12_embedding0_weight', 'selfattentivebilstm2_selfattentivebilstm12_lstm0_l0_i2h_weight', 'selfattentivebilstm2_selfattentivebilstm12_lstm0_l0_h2h_weight', ..., 'selfattentivebilstm2_selfattentivebilstm12_selfattention0_dense1_weight', 'selfattentivebilstm2_selfattentivebilstm12_selfattention0_dense1_bias', 'selfattentivebilstm2_selfattentivebilstm12_dense0_weight', 'selfattentivebilstm2_selfattentivebilstm12_dense0_bias'. Please make sure source and target networks have the same prefix."
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    model = gluon.nn.SymbolBlock.imports(\"model/att-symbol.json\", ['data'], \\\n",
    "                                         \"model/att-0001.params\", ctx=ctx)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = 'john is a former police, his daughter, sarah was killed in a car accident, which he \\\n",
    "        did not think was purely accidental. with his investigation went deeper, he found \\\n",
    "        that sarah\\'s boyfriend, jack, was the one to blame.'\n",
    "left = 'Word embedding can effectively represent the semantic similarity between words, \\\n",
    "        which brings many breakthroughs for natural language processing tasks. \\\n",
    "        The attention mechanism can intuitively grasp the important semantic features \\\n",
    "        in the sentence. I liked sarah and jack in this movie, but hate john, because he \\\n",
    "        killed sarah'\n",
    "left_token = vocab[tokenizer(left)]\n",
    "right_token = vocab[tokenizer(right)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[1.000000e+00 6.869653e-13]]\n",
       "<NDArray 1x2 @gpu(0)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_input = nd.array(left_token, ctx=ctx).reshape(1,-1)\n",
    "right_input = nd.array(right_token, ctx=ctx).reshape(1,-1)\n",
    "pred = model(left_input, right_input)\n",
    "pred\n",
    "#model.embedding_layer_right.weight.list_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to intuitively feel the role of the attention mechanism, we visualize the output of the model's attention on the predicted samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
