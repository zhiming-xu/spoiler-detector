{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Sequence Inference Model\n",
    "The model is based on [this paper](https://arxiv.org/abs/1609.06038).\n",
    "\n",
    "### Import Related Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import multiprocessing as mp\n",
    "import gluonnlp as nlp\n",
    "\n",
    "from mxnet import gluon, nd, init\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet import autograd, gluon, nd\n",
    "from d2l import try_gpu\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn's metric function to evaluate the results of the experiment\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# fixed random number seed\n",
    "np.random.seed(9102)\n",
    "mx.random.seed(9102)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "\n",
    "### Load Dataset\n",
    "\n",
    "See [dataloader](data_loader.ipynb) for how the training samples are obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder = 'data/imdb/'\n",
    "file_name = 'train.csv'\n",
    "file_path = data_folder + file_name\n",
    "# dev: True - only use a small dataset\n",
    "# create_vocab: True - create a new vocabulary from training data\n",
    "dev = True\n",
    "# load train file\n",
    "if dev:\n",
    "    # load only n rows\n",
    "    nrows = 5000\n",
    "    data = pd.read_csv(file_path, nrows=nrows)\n",
    "else:\n",
    "    # load as many as possible\n",
    "    data = pd.read_csv(file_path)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 4),     movie_id                                          sentence1  \\\n",
       " 0  tt0097576  The story opens in Monument Valley, Utah, in 1...   \n",
       " 1  tt2488496  Luke Skywalker has vanished. In his absence, t...   \n",
       " 2  tt0407887  In voiceover, Irish-American mobster Frank Cos...   \n",
       " 3  tt0209144  This is a complex story about Leonard Shelby (...   \n",
       " 4  tt1285016  In October 2003, Harvard University student Ma...   \n",
       " \n",
       "                                            sentence2  label  \n",
       " 0  Chemistry between Ford and Connery puts this f...  False  \n",
       " 1  You did it, J.J. ... You have ruined Star Wars...   True  \n",
       " 2  No respect! This remake of \"Internal Affairs\",...  False  \n",
       " 3  Scientific and psychological thriller. Who kne...   True  \n",
       " 4  A Grossly Overrated But Still Very Good Film A...  False  )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.shape, data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2num = {'contradiction':0, 'neutral':1, 'entailment':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 500)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a list of review a label paris.\n",
    "dataset = [[left, right, label] for left, right, label in \\\n",
    "           zip(data['sentence1'], data['sentence2'], data['label']) if label!='-' ]\n",
    "# randomly divide one percent from the training set as a verification set.\n",
    "train_dataset, valid_dataset = nlp.data.train_valid_split(dataset, valid_ratio=.1)\n",
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The film opens soon after the events of Casino Royale with Bond driving from Lake Garda to Siena, Italy. With the captured Mr. White in the luggage compartment of his car, Bond is attacked by pursuing henchmen. After evading his pursuers, and killing several of them, Bond arrives at an MI6 safe house.M updates Bond on Vesper\\'s boyfriend, Yusef Kabira, whose body had been found off the coast of Ibiza, however, the body is not Kabira\\'s, leading M to conclude that he\\'s still alive. Bond pockets a picture of his former lover Vesper Lynd and Kabira.Bond and M interrogate White regarding his secretive organization and it becomes apparent that MI6 knows little to nothing about them. White is quite amused and tells them that they \"have people everywhere,\" as in, right in this very room. At that moment, M\\'s bodyguard, Craig Mitchell, reveals his allegiance to the organization by shooting the MI6 guard and attempting to assassinate M. Bond thwarts the attempt by throwing a chair at Mitchell; allowing M to escape. As Bond struggles with Mitchell, a stray bullet from Mitchell\\'s gun hits White, who seems mortally wounded. Bond pursues Mitchell over several rooftops, eventually ending up in a building under renovation, momentarily causing chaos in the Palio di Siena horse race. A hand-to-hand fight ensues in a building containing renovation scaffolding. Bond and Mitchell lose their pistols. Bond is able to recover his own and kills Mitchell. When Bond returns to the safe house, he discovers White has escaped and the place deserted.Following a forensic investigation into Mitchell\\'s apartment and the discovery of marked American currency, Bond heads to Port au Prince, Haiti to track down Mitchell\\'s contact, Slate. Moments after Bond enters Slate\\'s hotel room, Slate attacks Bond, and Bond is forced to kill him. Bond assumes Slate\\'s identity and picks up a briefcase held for Slate at the front desk. As he exits the hotel, a Ford hatchback arrives and he is picked up by a woman named Camille Montes, who believes Bond is Slate. Examining the briefcase, Bond learns that Slate was sent to kill Montes at the behest of her lover, Dominic Greene, the chairman of an ecological organization called Greene Planet. Camille tries to shoot Bond and fails, and kicks him out of her car. Bond steals a motorcycle and follows her to the waterfront. While observing her meeting with Greene, Bond learns that Greene is helping a dangerous Bolivian general, Medrano, overthrow his government in exchange for a seemingly barren piece of desert. What he is unaware of is that Camille seeks revenge against Medrano after he murdered her family years ago.Greene has Camille escorted away on Medrano\\'s boat to \"sweeten\" their deal, but Bond rescues her. Bond then follows Greene to a private jet, which flies him to a lavish performance of Puccini\\'s Tosca at Lake Constance, Austria. On the plane, Greene meets with the CIA\\'s section chief for South America, Beam, and Felix Leiter, who, when asked if he recognizes Bond from a picture, says he does not. Greene also confirms a deal he\\'d made with Beam to control whatever resources his organization finds in or under the seemingly worthless piece of desert in Bolivia. Beam assumes that Greene has discovered oil there.Arriving in Austria, Bond discovers that Greene\\'s organization is named \"Quantum\" and several members are attending Tosca. Bond infiltrates Quantum\\'s meeting at the opera, stealing one of the member\\'s earpieces, and listens in on their conversation which concerns their sinister business dealings around the world. Bond, announcing that Quantum should probably \"find a more secure place to meet\", tricks them all into standing up to leave so he can take photos of them. He transmits the photos back to MI6 where M\\'s agents begin to identify them.As Bond leaves the theatre, a gunfight ensues in a restaurant. A bodyguard of Guy Haines, an adviser to the British Prime Minister, is killed by Greene\\'s men and Bond is framed. M has Bond\\'s passports and credit cards revoked because she believes Bond has killed too many potential sources of information. Bond travels to Italy to reunite with his old ally RenÃ© Mathis, whom he convinces to accompany him to La Paz, Bolivia, to investigate one of Greene\\'s business dealings there. On the flight over, Bond drink several glasses of his signature martini and broods over Vesper; he feels betrayed and heartbroken. In La Paz, they are greeted by Strawberry Fields, an MI6 field operative from the British Consulate, who demands that Bond return to the UK on the next available flight. Bond disobeys and, refusing to check into the seedy hotel Fields had chosen, checks into a luxury hotel with her (claiming to be teachers on sabbatical who have just won the lottery). Bond seduces her in their hotel suite.Bond meets Camille again at a ecological fund-raiser being held by Greene, where she is busy spoiling the fun of Greene\\'s party by pointing out his hypocrisy and lies to wealthy donors. Bond and Camille leave hastily together, but are pulled over by the Bolivian police. The police order Bond to pop the rear hatch of his car, revealing a bloodied and beaten Mathis. As Bond lifts Mathis out of the vehicle, the policemen shoot and fatally wound Mathis. After Bond subdues the police, he has a moment of relative tenderness with Mathis as the dying man asks Bond to stay with him. Mathis tells Bond that Vesper \"gave everything\" for him; his dying wish is that Bond forgive Vesper, and that he forgive himself.After depositing Mathis\\' body in a waste container, Bond and Camille drive to Greene\\'s intended land acquisition in the Bolivian desert and survey the area in a Douglas DC-3 plane. They are intercepted and shot down by an Aermacchi SF260 fighter and a Bell UH-1 Iroquois helicopter. They escape from the crippled plane by parachuting together into a sinkhole. While finding a route to the surface, Bond and Camille discover that Greene and Quantum is blockading Bolivia\\'s supply of fresh water, normally flowing in subterranean rivers, by damming it underground. Bond also discovers that Camille has spent years plotting revenge on General Medrano for the murder of her family and that Bond foiled that plan in Haiti when he rescued her. The two return to La Paz, where Bond meets M and learns Greene has killed Fields and left her sprawled on the bed, covered in petroleum. M tells Bond that Fields lungs are also full of the substance and that she was likely murdered by Quantum. Believing that Bond has become a threat to both friend and foe, and acting under higher orders, M orders him to disarm and end his activities in Bolivia. Bond escapes the agents who arrest him, defying M\\'s orders to surrender. M tells her men to watch him because she thinks he is \"on to something.\" Even though he has gone rogue, M confesses that she still has faith in him, and that he is \"still [her] agent.\" Before he leaves, Bond demands that M include in her report that Fields performed her duties to the best of her ability.By this point, both the American and British governments have agreed to work with Greene, because they think he has control of vast supplies of oil in Bolivia. Bond meets with Felix Leiter at a local bar. Like Bond, Felix thinks his government is on the wrong track. Leiter discloses that Greene and Medrano will meet at an eco-hotel, the Perla des las Dunas, in the Bolivian desert. Bond is forced to run when several CIA commandos suddenly appear and open fire.At the meeting in the hotel, Greene pays off the Bolivian Colonel of Police. Greene then threatens Gen. Medrano into signing a contract granting Greene\\'s company an overpriced proprietary utilities contract in Bolivia, which will be the only source for fresh water for the country. At first, Medrano refuses but Greene counters saying that Quantum is extremely powerful and influential, able to work with or topple any government or dictator and that Medrano could possibly be castrated and replaced with someone else if he does not agree to Greene\\'s demands. Medrano gruffly signs the document and leaves with the money.After the meeting, Bond kills off the Colonel of Police for betraying Mathis, and sets off a chain of explosions in the hotel when a hydrogen fuel tank is hit by an out of control vehicle. He battles hand-to-hand with Greene, who flees the hotel, limping from an injury to his foot. Camille foils Medrano\\'s attempted rape of a serving girl, and after a fight (during which Medrano tells Camille that she has the \"same frightened look\" her mother had before he killed her), Camille shoots Medrano. Bond rescues Camille from the burning building, and captures Greene. After interrogating him, he leaves Greene stranded in the middle of the desert with only a can of motor oil. Bond tells him that he bets Greene will make it 20 miles across the desert before he considers drinking the oil, contrasting the resources of oil and water. Bond drives Camille to a train station, where she muses on what life holds for her now that her revenge is complete. They kiss briefly but passionately before she departs.Bond travels to Kazan, Russia, where he finds Yusef Kabira. Yusef is revealed to be a member of Quantum who seduces high-ranking women with valuable connections, getting them to give up government assets as ransom for himself in fake kidnappings where he is supposedly held hostage. He\\'d previously tricked Vesper Lynd into the same sort of betrayal of MI6 and plans to do the same with Canadian agent Corinne Vaneau, even giving her the same kind of necklace he gave Vesper (an Algerian love knot). Surprising them at Yusef\\'s apartment, Bond tells Corinne about Vesper and advises her to alert the Canadian Security Intelligence Service. Bond leaves Yusef\\'s apartment and is confronted by M who is somewhat surprised that Bond did not kill Yusef, but rather left him alive for questioning. M reveals that Leiter has been promoted at the CIA to replace Beam, and that Greene was found dead in the desert, shot in the back of the head and with motor oil in his stomach. Bond doesn\\'t volunteer any information on Greene, but tells M that she was right about Vesper. M then tells Bond that MI6 needs him to come back to the agency. Bond walks off into the night telling M that he \"never left.\" As he leaves, he drops Vesper\\'s necklace in the snow.The traditional gun barrel sequence that opens nearly every film in the series (with the exception of this film\\'s predecessor, Casino Royale) appears just before the closing credits.',\n",
       " \"I should have listened to my friends I was warned that Quantum of Solace was one of the worst James Bond films, but I thought that I would give it a shot anyway. Oh how I regretted this decision.Quantum of Solace is just bad. Daniel Craig's James Bond is a clumsy fool, the film is bloated with explosions and action sequences and Dominic Greene was a crap villain. What got me was how SPOILERS Bond leaves him to his fate at the end of the film. What if Greene returned in the next film to wreak vengeance on Bond? (thankfully he doesn't') I bet bond would be feeling pretty stupid then.Read my full review here: http://goo.gl/leRsrk\",\n",
       " True]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing\n",
    "\n",
    "The purpose of the following code is to process the raw data so that the processed data can be used for model training and prediction. We will use the `SpacyTokenizer` to split the document into tokens, `ClipSequence` to crop the comments to the specified length, and build a vocabulary based on the word frequency of the training data. Then we attach the [Glove](https://nlp.stanford.edu/pubs/glove.pdf)[2]  pre-trained word vector to the vocabulary and convert each token into the corresponding word index in the vocabulary.\n",
    "Finally get the standardized training data set and verification data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Sequence clipping and get length Time=50.35s, #Sentences=4500\n",
      "Done! Sequence clipping and get length Time=37.99s, #Sentences=500\n"
     ]
    }
   ],
   "source": [
    "# tokenizer takes as input a string and outputs a list of tokens.\n",
    "tokenizer = nlp.data.SpacyTokenizer('en')\n",
    "length_review = 300\n",
    "length_plot = 600\n",
    "\n",
    "from src.util import mp_tokenizer\n",
    "\n",
    "tokenizer = mp_tokenizer(tokenizer, length_review, length_plot)\n",
    "\n",
    "# Preprocess the dataset\n",
    "train_dataset_token, train_data_lengths = tokenizer.process_dataset(train_dataset)\n",
    "valid_dataset_token, valid_data_lengths = tokenizer.process_dataset(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.util import mp_indexer\n",
    "embedding = 'glove.42B.300d'\n",
    "indexer = mp_indexer(train_dataset_token, embedding)\n",
    "train_dataset = indexer.process_dataset(train_dataset_token)\n",
    "valid_dataset = indexer.process_dataset(valid_dataset_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4500"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketing and DataLoader\n",
    "Since each sentence may have a different length, we need to use `Pad` to fill the sentences in a minibatch to equal lengths so that the data can be quickly tensored on the GPU. At the same time, we need to use `Stack` to stack the category tags of a batch of data. For convenience, we use `Tuple` to combine `Pad` and `Stack`.\n",
    "\n",
    "In order to make the length of the sentence pad in each minibatch as small as possible, we should make the sentences with similar lengths in a batch as much as possible. In light of this, we consider constructing a sampler using `FixedBucketSampler`, which defines how the samples in a dataset will be iterated in a more economic way.\n",
    "\n",
    "Finally, we use `DataLoader` to build a data loader for the training. dataset and validation dataset. The training dataset requires `FixedBucketSampler`, but the validation dataset doesn't require the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zhiming\\Miniconda3\\envs\\mx\\lib\\site-packages\\gluonnlp\\data\\batchify\\batchify.py:228: UserWarning: Padding value is not given and will be set automatically to 0 in data.batchify.Pad(). Please check whether this is intended (e.g. value of padding index in the vocabulary).\n",
      "  'Padding value is not given and will be set automatically to 0 '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=4500, batch_num=145\n",
      "  key=[57, 84, 111, 138, 165, 192, 219, 246, 273, 300]\n",
      "  cnt=[33, 59, 48, 86, 38, 41, 60, 12, 11, 4112]\n",
      "  batch_size=[84, 57, 43, 34, 32, 32, 32, 32, 32, 32]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "bucket_num = 10\n",
    "bucket_ratio = 0.5\n",
    "\n",
    "\n",
    "def get_dataloader():\n",
    "    # Construct the DataLoader pad data, stack label and lengths\n",
    "    batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(axis=0), \\\n",
    "                                          nlp.data.batchify.Pad(axis=0),\n",
    "                                          nlp.data.batchify.Stack())\n",
    "\n",
    "    # n this example, we use a FixedBucketSampler,\n",
    "    # which assigns each data sample to a fixed bucket based on its length.\n",
    "    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n",
    "        train_data_lengths,\n",
    "        batch_size=batch_size,\n",
    "        num_buckets=bucket_num,\n",
    "        ratio=bucket_ratio,\n",
    "        shuffle=True)\n",
    "    print(batch_sampler.stats())\n",
    "\n",
    "    # train_dataloader\n",
    "    train_dataloader = gluon.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_sampler=batch_sampler,\n",
    "        batchify_fn=batchify_fn)\n",
    "    # valid_dataloader\n",
    "    valid_dataloader = gluon.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        batchify_fn=batchify_fn)\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n",
    "train_dataloader, valid_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Weighted softmax cross entropy loss\n",
    "For a regular cross entropy loss, it is calculated by $-\\vec{y}^\\mathsf{T}\\log \\hat{\\vec{y}}$. Since $\\vec{y}$ is actually one-hot, so the value is $-\\log\\hat{y}_i$ where $i$ is the true label's index. To apply weight on this label, we multiply it with some weight value $w_i$, therefore, the loss of class $i$ is $-w_i\\log\\hat{y}_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSoftmaxCE(nn.HybridBlock):\n",
    "    def __init__(self, sparse_label=True, from_logits=False,  **kwargs):\n",
    "        super(WeightedSoftmaxCE, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.sparse_label = sparse_label\n",
    "            self.from_logits = from_logits\n",
    "\n",
    "    def hybrid_forward(self, F, pred, label, class_weight, depth=None):\n",
    "        if self.sparse_label:\n",
    "            label = F.reshape(label, shape=(-1, ))\n",
    "            label = F.one_hot(label, depth)\n",
    "        if not self.from_logits:\n",
    "            pred = F.log_softmax(pred, -1)\n",
    "\n",
    "        weight_label = F.broadcast_mul(label, class_weight)\n",
    "        loss = -F.sum(pred * weight_label, axis=-1)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignAttention(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AlignAttention, self).__init__(**kwargs)\n",
    "    \n",
    "    def forward(self, inp_left, inp_right):\n",
    "        # input dimension is of (batch_size, seq_len, embed_size)\n",
    "        # att dimension is of (batch_size, seq_len_left, seq_len_right)\n",
    "        att = nd.batch_dot(inp_left, nd.transpose(inp_right, axes = (0, 2, 1)))\n",
    "        # inp_left_dot dimention is of (batch_size, seq_left, embed_size)\n",
    "        inp_left_dot = nd.batch_dot(nd.softmax(att, axis=-1), inp_right)\n",
    "        # inp_right_dot dimension is of (batch_size, seq_right, embed_size)\n",
    "        inp_right_dot = nd.batch_dot(nd.softmax(nd.transpose(att, axes=(0, 2, 1)), axis=-1), inp_left)\n",
    "        # concat original (lstm output, dot multiplier, substraction, elementwise product)\n",
    "        # therefore, the real size is (batch_size, seq_len_left/right, embed_size*4)\n",
    "        aug_left = nd.concat(inp_left, inp_left_dot, inp_left-inp_left_dot, inp_left*inp_left_dot, dim=-1)\n",
    "        aug_right = nd.concat(inp_right, inp_right_dot, inp_right-inp_right_dot, inp_right*inp_right_dot, dim=-1)\n",
    "        return aug_left, aug_right, att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedSeqInfer(nn.Block):\n",
    "    def __init__(self, vocab_len, embsize, nhidden, nlayers, nfc, nclass, drop_prob, **kwargs):\n",
    "        super(EnhancedSeqInfer, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # dropout prob\n",
    "            self.drop_prob = drop_prob\n",
    "            # word embedding\n",
    "            self.embedding_layer = nn.Embedding(vocab_len, embsize)\n",
    "            # first lstm, from sentence embed to hidden outputs\n",
    "            self.bilstm1 = rnn.LSTM(nhidden, num_layers=nlayers, dropout=drop_prob, bidirectional=True)\n",
    "            # second lstm, from augmented embed to m\n",
    "            self.bilstm2 = rnn.LSTM(nhidden, num_layers=1, dropout=drop_prob, bidirectional=True)\n",
    "            # enhancement\n",
    "            self.align_att = AlignAttention()\n",
    "            # this layer is used to output the final class\n",
    "            self.output_layer = nn.HybridSequential()\n",
    "            self.output_layer.add(nn.Dense(nfc, activation='tanh'), nn.Dropout(rate=drop_prob), \\\n",
    "                                  nn.Dense(nfc, activation='tanh'), nn.Dropout(rate=drop_prob), \\\n",
    "                                  nn.Dense(nclass))\n",
    "\n",
    "    def forward(self, inp_left, inp_right):\n",
    "        # inp is a list containing left_text and right_text\n",
    "        # their size: [batch, token_idx]\n",
    "        # inp_embed_left/right size: [batch, seq_len, embed_size]\n",
    "        inp_embed_left = self.embedding_layer(inp_left)\n",
    "        inp_embed_right = self.embedding_layer(inp_right)\n",
    "        # rnn requires the first dimension to be the time steps, output is (seq_len, batch_size, embed_size)\n",
    "        h_output_left = self.bilstm1(nd.transpose(inp_embed_left, axes=(1, 0, 2)))\n",
    "        h_output_right = self.bilstm1(nd.transpose(inp_embed_right, axes=(1, 0, 2)))\n",
    "        m_left, m_right, att = self.align_att(nd.transpose(h_output_left, axes=(1, 0, 2)), \\\n",
    "                                                      nd.transpose(h_output_right, axes=(1, 0, 2)))\n",
    "        # apply another layer of lstm\n",
    "        # v_left/right shape is (seq_len, batch_size, embed_size)\n",
    "        v_left = self.bilstm2(nd.transpose(m_left, axes=(1, 0, 2)))\n",
    "        v_right = self.bilstm2(nd.transpose(m_right, axes=(1, 0, 2)))\n",
    "        # restore v's shape (batch_size, seq_len, embed_size)\n",
    "        v_left = nd.transpose(v_left, axes=(1, 0, 2))\n",
    "        v_right = nd.transpose(v_right, axes=(1, 0, 2))\n",
    "        # apply max pooling 1D and avg pooling 1D\n",
    "        v_left_avg = nd.sum(v_left, axis=1) / v_left.shape[1]\n",
    "        v_right_avg = nd.sum(v_right, axis=1) / v_right.shape[1]\n",
    "        v_left_max = nd.max(v_left, axis=1)\n",
    "        v_right_max = nd.max(v_right, axis=1)\n",
    "        # concatenate these 4 matrices\n",
    "        dense_input = nd.concat(v_left_avg, v_left_max, v_right_avg, v_left_max, dim=-1)\n",
    "        \n",
    "        output = self.output_layer(dense_input)\n",
    "        return output, att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure parameters and build models\n",
    "`test_model` is set to `True` if we want to just load parameters and test on the model. Otherwise, it is set to `False` during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnhancedSeqInfer(\n",
      "  (embedding_layer): Embedding(40004 -> 300, float32)\n",
      "  (bilstm1): LSTM(None -> 300, TNC, num_layers=2, dropout=0.5, bidirectional)\n",
      "  (bilstm2): LSTM(None -> 300, TNC, dropout=0.5, bidirectional)\n",
      "  (align_att): AlignAttention(\n",
      "  \n",
      "  )\n",
      "  (output_layer): HybridSequential(\n",
      "    (0): Dense(None -> 256, Activation(tanh))\n",
      "    (1): Dropout(p = 0.5, axes=())\n",
      "    (2): Dense(None -> 256, Activation(tanh))\n",
      "    (3): Dropout(p = 0.5, axes=())\n",
      "    (4): Dense(None -> 2, linear)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test_model = False\n",
    "vocab_len = len(indexer.vocab)\n",
    "emsize = 300    # word embedding size\n",
    "nhidden = 300   # lstm hidden_dim\n",
    "nlayers = 2     # lstm layers\n",
    "\n",
    "# final fc layer's number of hidden units and predicted number of classes\n",
    "nfc = 256\n",
    "nclass = 2\n",
    "\n",
    "drop_prob = 0.5\n",
    "\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "model = EnhancedSeqInfer(vocab_len, emsize, nhidden, nlayers, nfc, nclass, drop_prob)\n",
    "\n",
    "if test_model:\n",
    "    model.load_parameters('model/esim-0.79.params', ctx=ctx)\n",
    "else:\n",
    "    model.initialize(init=init.Xavier(), ctx=ctx)\n",
    "# Attach a pre-trained glove word vector to the embedding layer\n",
    "model.embedding_layer.weight.set_data(indexer.vocab.embedding.idx_to_vec)\n",
    "# fixed the embedding layer\n",
    "model.embedding_layer.collect_params().setattr('grad_req', 'null')\n",
    "\n",
    "print(model)\n",
    "\n",
    "train_curve, valid_curve = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(x_left, x_right, y, model, loss, class_weight):\n",
    "    pred, att = model(x_left, x_right)\n",
    "    y = nd.array(y.astype('int32', copy=False), ctx=ctx)\n",
    "    if loss_name == 'sce':\n",
    "        l = loss(pred, y)\n",
    "    elif loss_name == 'wsce':\n",
    "        l = loss(pred, y, class_weight, class_weight.shape[0])\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "    return pred, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(data_iter, model, loss, trainer, ctx, is_train, epoch, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    loss_val = 0.\n",
    "    total_pred = []\n",
    "    total_true = []\n",
    "    n_batch = 0\n",
    "\n",
    "    for batch_x_left, batch_x_right, batch_y in data_iter:\n",
    "        batch_x_left = batch_x_left.as_in_context(ctx)\n",
    "        batch_x_right = batch_x_right.as_in_context(ctx)\n",
    "        batch_y = batch_y.as_in_context(ctx)\n",
    "\n",
    "        if is_train:\n",
    "            with autograd.record():\n",
    "                batch_pred, l = calculate_loss(batch_x_left, batch_x_right, \\\n",
    "                                               batch_y, model, loss, class_weight)\n",
    "\n",
    "            # backward calculate\n",
    "            l.backward()\n",
    "\n",
    "            # update parmas\n",
    "            trainer.step(batch_x_left.shape[0])\n",
    "\n",
    "        else:\n",
    "            batch_pred, l = calculate_loss(batch_x_left, batch_x_right, \\\n",
    "                                           batch_y, model, loss, class_weight)\n",
    "\n",
    "        # keep result for metric\n",
    "        batch_pred = nd.argmax(nd.softmax(batch_pred, axis=1), axis=1).asnumpy()\n",
    "        batch_true = np.reshape(batch_y.asnumpy(), (-1, ))\n",
    "        total_pred.extend(batch_pred.tolist())\n",
    "        total_true.extend(batch_true.tolist())\n",
    "        \n",
    "        batch_loss = l.mean().asscalar()\n",
    "\n",
    "        n_batch += 1\n",
    "        loss_val += batch_loss\n",
    "\n",
    "        # check the result of traing phase\n",
    "        if is_train and n_batch % 400 == 0:\n",
    "            print('epoch %d, batch %d, batch_train_loss %.4f, batch_train_acc %.3f' %\n",
    "                  (epoch, n_batch, batch_loss, accuracy_score(batch_true, batch_pred)))\n",
    "\n",
    "    # metric\n",
    "    F1 = f1_score(np.array(total_true), np.array(total_pred), average='binary')\n",
    "    acc = accuracy_score(np.array(total_true), np.array(total_pred))\n",
    "    loss_val /= n_batch\n",
    "\n",
    "    if is_train:\n",
    "        print('epoch %d, learning_rate %.5f \\n\\t train_loss %.4f, acc_train %.3f, F1_train %.3f, ' %\n",
    "              (epoch, trainer.learning_rate, loss_val, acc, F1))\n",
    "        train_curve.append((acc, F1))\n",
    "        # declay lr\n",
    "        if epoch % 2 == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.9)\n",
    "    else:\n",
    "        print('\\t valid_loss %.4f, acc_valid %.3f, F1_valid %.3f, ' % (loss_val, acc, F1))\n",
    "        valid_curve.append((acc, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid(data_iter_train, data_iter_valid, model, loss, trainer, \\\n",
    "                ctx, nepochs, class_weight=None, loss_name='sce'):\n",
    "\n",
    "    for epoch in range(1, nepochs+1):\n",
    "        start = time.time()\n",
    "        # train\n",
    "        is_train = True\n",
    "        one_epoch(data_iter_train, model, loss, trainer, ctx, is_train, epoch, class_weight, loss_name)\n",
    "\n",
    "        # valid\n",
    "        is_train = False\n",
    "        one_epoch(data_iter_valid, model, loss, trainer, ctx, is_train, epoch, class_weight, loss_name)\n",
    "        end = time.time()\n",
    "        print('time %.2f sec' % (end-start))\n",
    "        print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = None\n",
    "loss_name = 'sce'\n",
    "optim_name = 'adam'\n",
    "lr = 0.001\n",
    "clip = 2.5\n",
    "nepochs = 10\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), optim_name, {'learning_rate': lr, 'clip_gradient': clip})\n",
    "\n",
    "if loss_name == 'sce':\n",
    "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "elif loss_name == 'wsce':\n",
    "    loss = WeightedSoftmaxCE()\n",
    "    # the value of class_weight is obtained by counting data in advance. It can be seen as a hyperparameter.\n",
    "    class_weight = nd.array([1., 3.], ctx=ctx)\n",
    "else:\n",
    "    print('loss function {} is not implemented!'.format(loss_name))\n",
    "    raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "MXNetError",
     "evalue": "[22:21:59] c:\\jenkins\\workspace\\mxnet-tag\\mxnet\\3rdparty\\mshadow\\mshadow\\./tensor_gpu-inl.h:69: Check failed: e == cudaSuccess: CUDA: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-3f359aa286e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# train and valid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m train_valid(train_dataloader, valid_dataloader, model, loss, \\\n\u001b[1;32m----> 3\u001b[1;33m             trainer, ctx, nepochs, class_weight=class_weight, loss_name=loss_name)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-94bd8d4a43cb>\u001b[0m in \u001b[0;36mtrain_valid\u001b[1;34m(data_iter_train, data_iter_valid, model, loss, trainer, ctx, nepochs, class_weight, loss_name)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mis_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mone_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_iter_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# valid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-849f1cb604e5>\u001b[0m in \u001b[0;36mone_epoch\u001b[1;34m(data_iter, model, loss, trainer, ctx, is_train, epoch, class_weight, loss_name)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# keep result for metric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mbatch_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mbatch_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mtotal_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\mx\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1994\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1995\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1996\u001b[1;33m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[0;32m   1997\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\mx\\lib\\site-packages\\mxnet\\base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    251\u001b[0m     \"\"\"\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mMXNetError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMXGetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMXNetError\u001b[0m: [22:21:59] c:\\jenkins\\workspace\\mxnet-tag\\mxnet\\3rdparty\\mshadow\\mshadow\\./tensor_gpu-inl.h:69: Check failed: e == cudaSuccess: CUDA: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "# train and valid\n",
    "train_valid(train_dataloader, valid_dataloader, model, loss, \\\n",
    "            trainer, ctx, nepochs, class_weight=class_weight, loss_name=loss_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model and the training, validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_parameters('model/esim-{:.4}.params'.format(str(valid_curve[-1][0])))\n",
    "with open('acc_record', 'w') as f:\n",
    "    f.write(str(train_curve)+'\\n')\n",
    "    f.write(str(valid_curve))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the training and validation curves\n",
    "We can find in the plot that after how many epochs the model performs best on validation set without overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "with open('acc_record', 'r') as f:\n",
    "    train_curve, valid_curve = [ literal_eval(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = [ acc for acc, _ in train_curve ]\n",
    "train_f1 = [ f1 for _, f1 in train_curve ]\n",
    "valid_acc = [ acc for acc, _ in valid_curve ]\n",
    "valid_f1 = [ f1 for _, f1 in valid_curve ]\n",
    "epochs = [ i for i in range(1, len(valid_curve)+1) ]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Accuracy')\n",
    "plt.plot(epochs, train_acc, label='train', color='orange')\n",
    "plt.plot(epochs, valid_acc, label='validation', color='green')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "plt.title('F1')\n",
    "plt.plot(epochs, train_f1, label='train', color='orange')\n",
    "plt.plot(epochs, valid_f1, label='validation', color='green')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on randomly made up plot and review by myself\n",
    "Now we will randomly input a movie plot summary and its review to see if the model decide the review is a spoiler or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = 'Tim killed Tom on a winter evening, and Sarah saw this scene.'\n",
    "left = 'Sarah saw Tom killed, it is Tim to blame'\n",
    "left_token = indexer.vocab[tokenizer.tokenizer(left)]\n",
    "right_token = indexer.vocab[tokenizer.tokenizer(right)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_input = nd.array(left_token, ctx=ctx).reshape(1,-1)\n",
    "right_input = nd.array(right_token, ctx=ctx).reshape(1,-1)\n",
    "pred, att = model(left_input, right_input)\n",
    "pred, nd.argmax(pred, axis=1).asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the soft alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "vis_att = np.squeeze(att.asnumpy())\n",
    "plt.figure(figsize=vis_att.T.shape)\n",
    "sns.heatmap(vis_att, xticklabels=tokenizer.tokenizer(right), yticklabels=tokenizer.tokenizer(left), center=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
